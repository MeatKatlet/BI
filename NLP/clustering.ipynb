{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Кирилл\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import  Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import chi2_contingency\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from NLP.supervized_clustering_metrics import precison_recall_f1\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "#import Stemmer\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем файл data.json - это список всех имен товаров, предварительно очищеный от стоп слов и лемматизированный.\n",
    "\n",
    "Имеем файл test_task.json - это сгруппированые товары - всего 1223 группы. Как проводилась группировка неизвестно, возможно это делали люди, возможно какой либо алгоритм.\n",
    "\n",
    "Задача: предложить лучший способ кластеризации имен товаров.\n",
    "\n",
    "Для кластеризации имен товаров будем использовать tf-idf векторизацию. Далее матрицу этих векторов отправим в spherical k-means для кластеризации. Количество кластеров задаем 1223 как в файле с образцом группировки. Spherical k-means - потому что он использует как меру близости векторов cousine similarity, это native метрика для близости векторов. Обычный k-means использует евклидово расстояние. Использование spherical k-means прибавляет к качеству кластеризации в данном случае 0.01%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Кирилл\\\\PycharmProjects\\\\BI\\\\NLP\\\\data.json') as f:\n",
    "    data = json.load(f)\n",
    "prep_docs = json.loads(data)\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "documents2 = []\n",
    "for doc in prep_docs:\n",
    "    doc2 = doc.lower()\n",
    "    tokens = [word for word in nltk.word_tokenize(doc2)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('/', token):\n",
    "           tt = token.split(\"/\")\n",
    "           tot = len(tt)\n",
    "           kk=0\n",
    "           for k in range(0,tot,1):\n",
    "               if (len(tt[k-kk]) <= 1):\n",
    "                    del tt[k-kk]\n",
    "                    kk += 1\n",
    "           token = \" \".join(tt)\n",
    "           #filtered_tokens.append(\"\".join(tt))\n",
    "\n",
    "        if len(token)>1:\n",
    "            filtered_tokens.append(token)\n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = 1\n",
    "            else:\n",
    "                dictionary[token] += 1\n",
    "    # stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    # stems2 = [stemmer2.stem(t) for t in stems]\n",
    "\n",
    "    sentence = ' '.join(filtered_tokens)\n",
    "    documents2.append(sentence)\n",
    "\n",
    "\n",
    "prep_docs = documents2\n",
    "documents2 = []\n",
    "for doc in prep_docs:\n",
    "    doc2 = doc.lower()\n",
    "    tokens = [word for word in nltk.word_tokenize(doc2)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "\n",
    "        if token not in dictionary \\\n",
    "                or token==\"зл\" or token==\"сие\" or token==\"лен\" or token == \"упак\" or token == \"ав\" or token == \"ава\":\n",
    "            continue\n",
    "        if dictionary[token]==1:\n",
    "            continue\n",
    "\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "    # stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    # stems2 = [stemmer2.stem(t) for t in stems]\n",
    "\n",
    "    sentence = ' '.join(filtered_tokens)\n",
    "    documents2.append(sentence)\n",
    "\n",
    "prep_docs = documents2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Убрали мусорные токены - обрывки слов. Разделили слова соединенные через слеш. Убрали слова с частотой в корпусе равной единице. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\Кирилл\\\\PycharmProjects\\\\BI\\\\NLP\\\\test_task.json\",orient='columns', encoding='utf-8')\n",
    "\n",
    "clusters = df.shape[0]\n",
    "max_group_size = df.shape[1]\n",
    "\n",
    "sentenceLabeled = []\n",
    "#df[:1]\n",
    "\n",
    "before = {}\n",
    "before_sent = {}\n",
    "\n",
    "documents = []\n",
    "\n",
    "sentenceID = 0\n",
    "b = np.array([])\n",
    "for group in range(0,clusters,1):\n",
    "    for col in range(0, max_group_size, 1):\n",
    "        if (df[col][group] != None):\n",
    "            #sentenceL = TaggedDocument(words=df[col][group].lower().split(), tags=['SENT_%s' % sentenceID])\n",
    "            # sentenceL = TaggedDocument(words=sentence.split(), tags = ['SENT_%s' %sentenceID])\n",
    "            #sentenceLabeled.append(sentenceL)\n",
    "            #before['SENT_%s' % sentenceID] = group\n",
    "            #before_sent['SENT_%s' % sentenceID] = df[col][group]\n",
    "            #documents.append(df[col][group])\n",
    "            b = np.append(b, int(group))\n",
    "            sentenceID += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "переменная b - это массив в котором значения - принадлежность товара к группе в данном нам файле, значения массива - номер группы к которой принадлежит этот товар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer=\"word\", max_df=0.8)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(prep_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_components(var_ratio, goal_var: float):\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "\n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "\n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "\n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "\n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "\n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "\n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "svd = TruncatedSVD(tfidf_matrix.shape[1]-1)\n",
    "svd.fit(tfidf_matrix)\n",
    "\n",
    "tsvd_var_ratios = svd.explained_variance_ratio_\n",
    "n = select_n_components(tsvd_var_ratios, 0.98)\n",
    "svd = TruncatedSVD(n)\n",
    "\n",
    "#todo добавить функцию делающую количество компонент связности чтобы потом вставить её в TruncatedSVD\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Манипуляции выше нужны чтобы сделать sperical k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=clusters)\n",
    "km.fit(X)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем метрики для кластеризации как таковой. А так же метрики с учетом true классов: precison, recall, f1. Для этого написана одноименная функция по мотивам https://stats.stackexchange.com/questions/15158/precision-and-recall-for-clustering?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.935\nCompleteness: 0.939\nV-measure: 0.937\nAdjusted Rand-Index: 0.532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_doc is 3580 \npositives is 4938 \nTP is 2528 \nFP is 2410 \nFN is 2030 \nTN is 6399442 \nPrecision is 0.51 \nRecall is 0.55 \nF1 is 0.53 \n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(b, clusters))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(b, clusters))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(b, clusters))\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(b, clusters))\n",
    "\n",
    "indeces_of_groups = {}\n",
    "dict_list = []\n",
    "for i in range(0,len(clusters),1):\n",
    "    val = clusters[i]\n",
    "    if val not in indeces_of_groups:\n",
    "        indeces_of_groups[val] = []\n",
    "        indeces_of_groups[val].append(str(b[i]))\n",
    "    else:\n",
    "        indeces_of_groups[val].append(str(b[i]))\n",
    "\n",
    "[dict_list.append(v) for k,v in indeces_of_groups.items()]\n",
    "\n",
    "precison_recall_f1(dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
